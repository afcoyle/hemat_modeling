# Some tidbits are listed as missing, but have tidbit numbers in tidbit_id
# Others don't have a value in tidbit_id, but are listed in the comments
# Let's make some manual corrections
pot_dat[grep("no tidbit", pot_dat$Pot.Comment, ignore.case = TRUE), ]$tidbit_id <- NA
pot_dat[grep("tidbit missing", pot_dat$Pot.Comment, ignore.case = TRUE), ]$tidbit_id <- NA
pot_dat[grep("Pot open, no sample, Tidbit broken", pot_dat$Pot.Comment, ignore.case = TRUE), ]$tidbit_id <- NA
pot_dat[grep("tidbit 10", pot_dat$Pot.Comment, ignore.case = TRUE), ]$tidbit_id <- 10
# Let's also check about values labeled "tb"
tidbit_pots <- pot_dat %>%
filter(grepl("TB", Pot.Comment, ignore.case = TRUE)) %>%
filter(Year > 2008)
# Yep, we've got a bunch. Bummer! Let's fix those. All are from 2009, so we'll specify
# Simplest way is gonna be just removing the 2009 data, editing it separately, then reading it back in
# That'll eliminate issues with, say, 2008 pots with "TB 2007" in the comments being read in as tidbit_id = 20
ohnine_pot_dat <- pot_dat %>%
filter(Year == 2009)
pot_dat <- pot_dat %>%
filter(Year != 2009)
new_digs <- str_extract(ohnine_pot_dat$Pot.Comment, "TB \\d{1,2}")
new_digs <- str_extract(new_digs, "\\d{1,2}")
for (i in 1:length(new_digs)){
if (!is.na(new_digs[i])) {
ohnine_pot_dat$tidbit_id[i] <- new_digs[i]
}
}
# Re-merge 2009 data in
pot_dat <- rbind(ohnine_pot_dat, pot_dat)
# Manually scan through comments from un-examined years to look for any unusual references to tidbits or loggers
test <- pot_dat %>%
filter(Year > 2009)
# None found! We can safely remove the comments column
pot_dat <- pot_dat %>%
select(-Pot.Comment)
# 2005
table(pot_dat[pot_dat$Year == 2005, ]$tidbit_id)
table(full_dat[full_dat$year == 2005, ]$tidbit_id)
# We've got a few that aren't present in the tidbit ID data (these are likely from the RKC survey), but all IDs in the tidbit data have a corollary in the pot data
# Searching for two things:
#   - pot_data tidbit ID values that appear to be duplicated, with both having a match to full_data tidbit ID values (ex: 2207 and 22-07)
#   - full_data values with errors in tidbit ID entries (ex: leading zeros)
# 2006
table(pot_dat[pot_dat$Year == 2006, ]$tidbit_id)
table(full_dat[full_dat$year == 2006, ]$tidbit_id)
# Oof, looks like a lot within the pot data are preceded by a #. Let's remove that from all throughout all years
pot_dat$tidbit_id <- sub("#", "", pot_dat$tidbit_id)
# Check again
names(table(pot_dat[pot_dat$Year == 2006, ]$tidbit_id))
names(table(full_dat[full_dat$year == 2006, ]$tidbit_id))
# Okay, we've still got issues. What's listed as Tidbit 2 in the tidbit data is listed as Tidbit 02 in the comments. This is an issue in many years, so let's just solve it all at once.
pot_dat$tidbit_id <- str_remove(pot_dat$tidbit_id, "^0")
# Check again
names(table(pot_dat[pot_dat$Year == 2006, ]$tidbit_id))
names(table(full_dat[full_dat$year == 2006, ]$tidbit_id))
# Looks good!
#2007
names(table(pot_dat[pot_dat$Year == 2007, ]$tidbit_id))
names(table(full_dat[full_dat$year == 2007, ]$tidbit_id))
# Looks good!
#2008
names(table(pot_dat[pot_dat$Year == 2008, ]$tidbit_id))
names(table(full_dat[full_dat$year == 2008, ]$tidbit_id))
# Alright, some tidbits from this year have multiple names. Let's modify
# Change 17-07 to 17
pot_dat[which(pot_dat$Year == 2008 & pot_dat$tidbit_id == "17-07"), ]$tidbit_id <- "17"
# Change 177 to 17
pot_dat[which(pot_dat$Year == 2008 & pot_dat$tidbit_id == "177"), ]$tidbit_id <- "17"
# Change 18-07 to 18
pot_dat[which(pot_dat$Year == 2008 & pot_dat$tidbit_id == "18-07"), ]$tidbit_id <- "18"
# Change 1807 to 18
pot_dat[which(pot_dat$Year == 2008 & pot_dat$tidbit_id == "1807"), ]$tidbit_id <- "18"
# Change 187 to 18
pot_dat[which(pot_dat$Year == 2008 & pot_dat$tidbit_id == "187"), ]$tidbit_id <- "18"
# Change 2-07 to 2
pot_dat[which(pot_dat$Year == 2008 & pot_dat$tidbit_id == "2-07"), ]$tidbit_id <- "2"
# Change 20-07 to 2007
pot_dat[which(pot_dat$Year == 2008 & pot_dat$tidbit_id == "20-07"), ]$tidbit_id <- "2007"
# Change 207 to 2
pot_dat[which(pot_dat$Year == 2008 & pot_dat$tidbit_id == "207"), ]$tidbit_id <- "2"
# Change 21-07, 2107, and 217 to 21
pot_dat[which(pot_dat$Year == 2008 & pot_dat$tidbit_id == "21-07" | pot_dat$tidbit_id == "2107" | pot_dat$tidbit_id == "217"), ]$tidbit_id <- "21"
# Change 22-07, 2207, and 227 to 21
pot_dat[which(pot_dat$Year == 2008 & pot_dat$tidbit_id == "22-07" | pot_dat$tidbit_id == "2207" | pot_dat$tidbit_id == "227"), ]$tidbit_id <- "22"
# Change 24-07, 2407, and 247 to 21
pot_dat[which(pot_dat$Year == 2008 & pot_dat$tidbit_id == "24-07" | pot_dat$tidbit_id == "2407" | pot_dat$tidbit_id == "247"), ]$tidbit_id <- "24"
# 2009
names(table(pot_dat[pot_dat$Year == 2009, ]$tidbit_id))
names(table(full_dat[full_dat$year == 2009, ]$tidbit_id))
# Looks good!
# 2010
names(table(pot_dat[pot_dat$Year == 2010, ]$tidbit_id))
names(table(full_dat[full_dat$year == 2010, ]$tidbit_id))
# Looks good!
# 2011
names(table(pot_dat[pot_dat$Year == 2011, ]$tidbit_id))
names(table(full_dat[full_dat$year == 2011, ]$tidbit_id))
# Looks like for Tanner survey, Leg 2, 5 got renamed 5_0.
# Let's fix
full_dat[which(full_dat$year == 2011 & full_dat$tidbit_id == "5_0"), ]$tidbit_id <- "5"
# Alright, now it looks good!
# 2012
names(table(pot_dat[pot_dat$Year == 2012, ]$tidbit_id))
names(table(full_dat[full_dat$year == 2012, ]$tidbit_id))
# Looks good!
# 2013
names(table(pot_dat[pot_dat$Year == 2013, ]$tidbit_id))
names(table(full_dat[full_dat$year == 2013, ]$tidbit_id))
# We've got some prefix zeroes in full_dat as well, let's remove those too
full_dat$tidbit_id <- str_remove(full_dat$tidbit_id, "^0")
# Looks good now!
# 2014
names(table(pot_dat[pot_dat$Year == 2014, ]$tidbit_id))
names(table(full_dat[full_dat$year == 2014, ]$tidbit_id))
# Looks good!
# 2015
names(table(pot_dat[pot_dat$Year == 2015, ]$tidbit_id))
names(table(full_dat[full_dat$year == 2015, ]$tidbit_id))
# Looks good!
# 2016
names(table(pot_dat[pot_dat$Year == 2016, ]$tidbit_id))
names(table(full_dat[full_dat$year == 2016, ]$tidbit_id))
# Looks good!
# 2017
names(table(pot_dat[pot_dat$Year == 2017, ]$tidbit_id))
names(table(full_dat[full_dat$year == 2017, ]$tidbit_id))
# Looks good!
# 2018
names(table(pot_dat[pot_dat$Year == 2018, ]$tidbit_id))
names(table(full_dat[full_dat$year == 2018, ]$tidbit_id))
# Looks good!
# 2019
names(table(pot_dat[pot_dat$Year == 2019, ]$tidbit_id))
names(table(full_dat[full_dat$year == 2019, ]$tidbit_id))
# Looks good!
# It won't work to merge both full dataframes at once (it's just too large - 400,000 rows x 10,000 rows)
# Instead, we'll try to merge each year individually using a for loop
# Initialize loop with matrix and vector of years
tidbit_pot_merged <- matrix(nrow = 0, ncol = 20)
years <- 2005:2019
for (i in 1:length(years)){
crab_year <- years[i]
# Filter pot data and tidbit data to only include a single year
year_pot <- pot_dat %>%
filter (Year == crab_year)
year_tidbit <- full_dat %>%
filter(year == crab_year)
# Join pot and tidbit data by tidbit ID
year_dat <- inner_join(year_pot, year_tidbit, by = "tidbit_id")
# Select only true entries by choosing rows where the data point is:
#   - 2 min before time hauled (to account for small timekeeping errors)
#   - 5 min after pot was set (to allow it to fall to the bottom)
# Create new column calculating difference between tidbit time and haul time
year_dat$change_time <- difftime(year_dat$tidbit_datetime, year_dat$Time.Hauled, tz = "US/Alaska", units = "mins")
# Filter to only include rows 60+ min before haul time in case of minor timekeeping errors
year_dat <- year_dat %>%
filter(change_time < -61)
# Change column to difference between tidbit time and set time
year_dat$change_time <- difftime(year_dat$tidbit_datetime, year_dat$Time.Set, tz = "US/Alaska", units = "mins")
# Filter to only include rows 60+ min after set time in case of minor timekeeping errors
year_dat <- year_dat %>%
filter(change_time > 61)
# Remove column
year_dat <- year_dat %>%
select(-change_time)
# Calculate average temperature, adding a new column in the database for it
year_dat <- year_dat %>%
group_by(Time.Hauled) %>%
mutate(mean_temp = mean(Temp)) %>%
ungroup
# We'll also calculate the difference between max and min temperatures. This will be a "red flag" column -
# entries with a large difference between max and min temps are likely to be the result of data entry errors.
# Calculate max temperature, adding a new column in the database for it
year_dat <- year_dat %>%
group_by(Time.Hauled) %>%
mutate(diff_temp = (max(Temp) - min(Temp))) %>%
ungroup
# Remove duplicate IDs from Time.Hauled. This leaves us with one row per haul. That eliminates temp (the specific measurements), date, time, and DateTime, but we don't care about that.
# It keeps average temperature of the pot, which is what we care about!
year_dat <- year_dat[!duplicated(year_dat$Time.Hauled), ]
# Append this to our merged data
tidbit_pot_merged <- rbind(tidbit_pot_merged, year_dat)
}
# First, we'll ditch any line with a difference in temperature greater than 3, as it's likely to be an error in readings or in data entry (for the pot ID or tidbit ID).
tidbit_pot_merged <- tidbit_pot_merged %>%
filter(diff_temp < 3)
# Next, we'll eliminate all pots with a set or haul time of 0:00:00. While I'm sure some may have been hauled or set at precisely midnight, it's more likely that the exact time was simply not entered (or present on the data forms), and thus our values are incorrect.
# Look at Time.Set column, eliminate rows set at 0:00:00
tidbit_pot_merged$time <- tidbit_pot_merged$Time.Set %>%
str_split(" ") %>%
map_chr(2) %>%
hms()
tidbit_pot_merged <- tidbit_pot_merged %>%
filter(time != "0S")
# Look at Time.hauled column, eliminate rows set at 0:00:00
tidbit_pot_merged$time <- tidbit_pot_merged$Time.Hauled %>%
str_split(" ") %>%
map_chr(2) %>%
hms()
tidbit_pot_merged <- tidbit_pot_merged %>%
filter(time != "0S")
# Drop the time column
tidbit_pot_merged <- tidbit_pot_merged %>%
select(-time)
# We'll now go through the top mean temperatures in search of data entry errors
# These are likely to be errors in noting which Tidbit is which, and thus can't really be searched for.
# We have to do it...sigh...manually
# 2014 RKC, Leg 3, Tidbit 22. Mean temp = 14.11C.
# First deployment of the leg, I think they forgot to add the Tidbit
# Following deployments were approx 6C, don't believe this was valid msmt
tidbit_pot_merged <- tidbit_pot_merged[as.character(tidbit_pot_merged$Time.Hauled) != "2014-07-23 10:29:00", ]
# 2011 Tanner crab, Leg 1
# Almost all our new highest temp values are from those set on 10-07 on this survey
# An examination of NOAA weather reports (at https://www.ndbc.noaa.gov/station_history.php?station=jnea2) don't reveal any unusual warmth
# Furthermore, the pots experience a dramatic cold snap (substantially cooler than air temperature at the time) after they're lifted from the water
# Finally, no pots are listed as being set on 10-08 or pulled on  10-09 which is quite unusual (especially as pots were set on 10-09).
# Finally, pots set the next day are quite cool, typically around 4C.
# My hypothesis: the captain got their dates mixed up, and entered the pots as being set on the 7th and pulled on the 8th, whereas in reality they were
# set on the 8th and pulled on the 9th.
# Therefore, we'll remove all pots from this day and the one before.
tidbit_pot_merged$date <- tidbit_pot_merged$Time.Set %>%
str_split(" ") %>%
map_chr(1)
tidbit_pot_merged <- tidbit_pot_merged %>%
filter(date != "2011-10-07")
# 2016 RKC, Leg 3
# Our highest three mean temperatures are all from this survey, specifically from pots set on July 25th (tidbits 16, 7, and 13).
# All temperatures are approx. 9.3C
# However, these look quite legitimate. In all cases, the set time and tidbit data agree. Prior to the pot being set, the tidbit data gives temps
# around 13-14C. And after the pot was hauled, the tidbit data temps return to 13-14C. Therefore, it's quite likely that these temperatures are accurate.
# 2013 Tanner crab, Leg 2
# Most of the remaining top 20 or so temp values are from those set at 10-20 on this survey.
# However, these look legitimate as well. The temps are cooler than those from 2011 Tanner, Leg 2, plus the pots don't experience a cooling after being hauled. # Instead, they warm slightly. An example of this is Tidbit 20, which is approx. 9C from 8pm on Oct 19th until 11am on Oct 20th. This is then followed # by a few hours at approx 11C, then at 1pm it drops to 9C again. This corresponds with the pot data pattern of sets + hauls (set at 8pm on the 19th, hauled at 11am on the 20th, set again at 1pm on the 20th).
# NOAA sea surface temperatures are warm at this time in Juneau (7.7C), and could be warmer at the nearby Port Camden, where these pots were taken.
# Given that they align broadly with the pot data pattern of sets + hauls, and these temperatures are seen across many Tidbits, it's quite likely that these temperatures are accurate.
# Remove unnecessary columns we've created, plus some additional unneeded columns
tidbit_pot_merged <- tidbit_pot_merged %>%
select(-c(year, survey, tidbit_datetime, Temp, diff_temp, date, Density.Strata, Time.Set))
# Change column names
tidbit_pot_merged <- tidbit_pot_merged %>%
rename(temp = mean_temp)
# Convert
# Write out data to file
write.csv(tidbit_pot_merged, file = "../output/ADFG_SE_AK_pot_surveys/cleaned_data/pot_data_with_temperature.csv",
row.names = FALSE)
knitr::opts_chunk$set(echo = TRUE)
# Add all required libraries here
list.of.packages <- c("tidyverse", "lubridate")
# Get names of all required packages that aren't installed
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[, "Package"])]
# Install all new packages
if(length(new.packages)) install.packages(new.packages)
# Load all required libraries
lapply(list.of.packages, FUN = function(X) {
do.call("require", list(X))
})
# Examine things related to crabs (1 row per crab sampled)
crab_dat <- read.csv(file = "../output/ADFG_SE_AK_pot_surveys/cleaned_data/crab_data/BCS_examined_crab_with_temperature.csv")
# Examine things related to location (1 row per pot deployment)
pot_dat <- read.csv(file = "../output/ADFG_SE_AK_pot_surveys/cleaned_data/pot_data_with_temperature.csv")
#### Temperature and Site --------------------------
exper <- pot_dat %>%
group_by(Location) %>%
summarize(temp_avg  = mean(temp),
temp_sd = sd(temp),
n_pots = n()) %>%
ungroup()
ggplot(exper) +
geom_bar(aes(x = Location, y = temp_avg),
stat = "identity") +
geom_errorbar(aes(x = Location, ymin = temp_avg - temp_sd, ymax = temp_avg + temp_sd)) +
theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
# Alright, neat little graph here! Looks like most sites have a broadly similar range of temperatures, with some exceptions. Port Camden in particular seems to be substantially higher than practically every other site!
# Let's look at how temperature relates to year!
exper <- pot_dat %>%
group_by(Year) %>%
summarize(temp_avg  = mean(temp),
temp_sd = sd(temp),
n_pots = n()) %>%
ungroup()
ggplot(exper) +
geom_bar(aes(x = Year, y = temp_avg),
stat = "identity") +
geom_errorbar(aes(x = Year, ymin = temp_avg - temp_sd, ymax = temp_avg + temp_sd)) +
theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
#### Temp vs. time of year ----------------------------
# We'll use Julian day for this
exper <- pot_dat
exper$Jul.Day <- as.Date(exper$Time.Hauled)
exper$Jul.Day <- yday(exper$Jul.Day)
# First, a scatterplot of all temps and Julian days
ggplot(exper, aes(x = Jul.Day, y = temp)) +
geom_point()
# Alright, we can see an overall gentle curve, along with the timing of each survey (Tanner vs. RKC)
# Now, let's make a line plot!
exper <- exper %>%
group_by(Jul.Day) %>%
summarize(temp_avg = mean(temp))
ggplot(exper, aes( x= Jul.Day, y = temp_avg)) +
geom_line() +
geom_point()
# Now, let's make a line plot!
expersum <- exper %>%
group_by(Jul.Day) %>%
summarize(temp_avg = mean(temp))
#### Temp vs. time of year ----------------------------
# We'll use Julian day for this
exper <- pot_dat
exper$Jul.Day <- as.Date(exper$Time.Hauled)
exper$Jul.Day <- yday(exper$Jul.Day)
# First, a scatterplot of all temps and Julian days
ggplot(exper, aes(x = Jul.Day, y = temp)) +
geom_point()
# Now, let's make a line plot!
expersum <- exper %>%
group_by(Jul.Day) %>%
summarize(temp_avg = mean(temp))
#### Temp vs. time of year ----------------------------
# We'll use Julian day for this
pot_dat$Jul.Day <- as.Date(pot_dat$Time.Hauled)
pot_dat$Jul.Day <- yday(pot_dat$Jul.Day)
# First, a scatterplot of all temps and Julian days
ggplot(pot_dat, aes(x = Jul.Day, y = temp)) +
geom_point()
# Now, let's make a line plot!
exper <- pot_dat %>%
group_by(Jul.Day) %>%
summarize(temp_avg = mean(temp))
ggplot(exper, aes( x= Jul.Day, y = temp_avg)) +
geom_line() +
geom_point()
#### Survey Date and Year ---------------------------------
exper <- pot_dat %>%
group_by(c(Jul.Day, Year))
group_by
?group_by
#### Survey Date and Year ---------------------------------
exper <- pot_dat %>%
group_by(Jul.Day, Year)
exper
#### Survey Date and Year ---------------------------------
exper <- pot_dat %>%
group_by(Trip.Start.Date)
exper
#### Survey Date and Year ---------------------------------
ggplot(pot_dat, aes(x = Year, y = Trip.Start.Date))
#### Survey Date and Year ---------------------------------
ggplot(pot_dat, aes(x = Year, y = Trip.Start.Date)) +
geom_point()
#### Survey Date and Year ---------------------------------
ggplot(pot_dat, aes(x = Jul.Day, y = Trip.Start.Date)) +
geom_point()
# Add all required libraries here
list.of.packages <- c("tidyverse", "lubridate", "ggridges")
# Get names of all required packages that aren't installed
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[, "Package"])]
# Install all new packages
if(length(new.packages)) install.packages(new.packages)
# Load all required libraries
lapply(list.of.packages, FUN = function(X) {
do.call("require", list(X))
})
#### Survey Date and Year ---------------------------------
# Create table of years of each survey
dates <- matrix(data = NA, nrow = 0, ncol = 4)
a <- 1:365
View(pot_dat)
# Add all required libraries here
list.of.packages <- c("tidyverse", "lubridate", "ggridges", "tidyquant")
# Add all required libraries here
list.of.packages <- c("tidyverse", "lubridate", "ggridges", "tidyquant")
# Get names of all required packages that aren't installed
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[, "Package"])]
# Install all new packages
if(length(new.packages)) install.packages(new.packages)
# Load all required libraries
lapply(list.of.packages, FUN = function(X) {
do.call("require", list(X))
})
View(pot_dat)
#### Temp vs. depth ---------------------------
# We'll look at the average depth of each site vs. the average temp
exper <- pot_dat %>%
group_by(Location) %>%
summarize(depth_avg  = mean(Depth.Fathoms),
temp_avg = mean(temp),
temp_sd = sd(temp),
depth_sd = sd(Depth.Fathoms),
n_pots = n()) %>%
ungroup()
exper
# First, depth of each site
ggplot(exper) +
geom_bar(aes(x = Location, y = depth_avg),
stat = "identity") +
geom_errorbar(aes(x = Location, ymin = depth_avg - depth_sd, ymax = depth_avg + depth_sd)) +
theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
# Now, depth vs. temperature
ggplot(exper, aes(x = depth_avg, y = temp_avg)) +
geom_point()
# First, depth of each site
ggplot(exper) +
geom_bar(aes(x = Location, y = depth_avg),
stat = "identity") +
geom_errorbar(aes(x = Location, ymin = depth_avg - depth_sd, ymax = depth_avg + depth_sd)) +
theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
# Now let's switch it up to maximum depth (more of an examination of the water bodies) and temperature
exper <- pot_dat %>%
group_by(Location) %>%
summarize(depth_max  = max(Depth.Fathoms),
temp_avg = mean(temp),
temp_sd = sd(temp),
n_pots = n()) %>%
ungroup()
# First, depth of each site
ggplot(exper) +
geom_bar(aes(x = Location, y = depth_max),
stat = "identity") +
theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
# Now, depth vs. temperature
ggplot(exper, aes(x = depth_max, y = temp_avg)) +
geom_point()
# First, depth of each site
ggplot(exper) +
geom_bar(aes(x = Location, y = depth_max),
stat = "identity") +
theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
# Now, depth vs. temperature
ggplot(exper, aes(x = depth_max, y = temp_avg)) +
geom_point()
# First, depth of each site
ggplot(exper) +
geom_bar(aes(x = Location, y = depth_avg),
stat = "identity") +
geom_errorbar(aes(x = Location, ymin = depth_avg - depth_sd, ymax = depth_avg + depth_sd)) +
theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
#### Temp vs. depth ---------------------------
# We'll look at the average depth of each site vs. the average temp
exper <- pot_dat %>%
group_by(Location) %>%
summarize(depth_avg  = mean(Depth.Fathoms),
temp_avg = mean(temp),
temp_sd = sd(temp),
depth_sd = sd(Depth.Fathoms),
n_pots = n()) %>%
ungroup()
# First, depth of each site
ggplot(exper) +
geom_bar(aes(x = Location, y = depth_avg),
stat = "identity") +
geom_errorbar(aes(x = Location, ymin = depth_avg - depth_sd, ymax = depth_avg + depth_sd)) +
theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
# Now, depth vs. temperature
ggplot(exper, aes(x = depth_avg, y = temp_avg)) +
geom_point()
# Now let's switch it up to maximum depth (more of an examination of the water bodies) and temperature
exper <- pot_dat %>%
group_by(Location) %>%
summarize(depth_max  = max(Depth.Fathoms),
temp_avg = mean(temp),
temp_sd = sd(temp),
n_pots = n()) %>%
ungroup()
# First, depth of each site
ggplot(exper) +
geom_bar(aes(x = Location, y = depth_max),
stat = "identity") +
theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
# Now, depth vs. temperature
ggplot(exper, aes(x = depth_max, y = temp_avg)) +
geom_point()
View(exper)
View(crab_dat)
View(pot_dat)
# Now let's switch it up to maximum depth (more of an examination of the water bodies) and temperature
exper <- pot_dat %>%
group_by(Location) %>%
summarize(depth_max  = max(Depth.Fathoms),
temp_avg = mean(temp),
temp_sd = sd(temp),
n_pots = n(),
avg_day = mean(Jul.Day),
day_sd = sd(Jul.Day))%>%
ungroup()
# First, depth of each site
ggplot(exper) +
geom_bar(aes(x = Location, y = depth_max),
stat = "identity") +
theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
# Now, depth vs. temperature
ggplot(exper, aes(x = depth_max, y = temp_avg)) +
geom_point()
# What about average date each site was sampled?
ggplot(exper) +
geom_bar(aes(x = Location, y = avg_day),
stat = "identity") +
geom_errorbar(aes(x = Location, ymin = avg_day - day_sd, ymax = avg_day + day_sd)) +
theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
# Hmm, alright. Looks like a few of the sites are specific to the later survey, while most aren't.
# Let's verify this with a table
table(pot_dat$Location, pot_dat$Project)
View(pot_dat)
View(crab_dat)
View(pot_dat)
# One thing I'm concerned about is a strong relationship between temperature and date. Let's figure that out
corr(pot_dat$temp, pot_dat$Jul.Day)
# One thing I'm concerned about is a strong relationship between temperature and date. Let's figure that out
cor(pot_dat$temp, pot_dat$Jul.Day)
# Alright, that's acceptable! Let's just graph it too
ggplot(pot_dat, aes(x = Jul.Day, y = temp)) +
geom_point()
# Let's also double check on latitude. Almost definitely fine, because our spatial scale is small, but hey
ggplot(pot_dat, aes(x = Latitude.Decimal.Degrees, y = temp)) +
geom_point()

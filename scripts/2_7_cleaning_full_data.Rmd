---
title: "2_7_cleaning_full_data.Rmd"
author: "Aidan Coyle"
date: "8/4/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

In the previous script, we finished by creating a full dataframe containing temperature and catch data for each crab (excluding crabs without either data on their temperature or the presence/absence of Bitter Crab Syndrome).

Now it's time to start exploring the data! Let's check out what's been going on with these crabs, and see the relationship to temperature.

#### Load libraries (and install if necessary), and load packages

```{r libraries, message=FALSE, warning=FALSE}
# Add all required libraries here
list.of.packages <- c("tidyverse", "lubridate", "beepr")
# Get names of all required packages that aren't installed
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[, "Package"])]
# Install all new packages
if(length(new.packages)) install.packages(new.packages)


# Load all required libraries
lapply(list.of.packages, FUN = function(X) {
  do.call("require", list(X))
})
```

# Read in data and fix it up

```{r}
# Examine things related to crabs (1 row per crab sampled)
crab_dat <- read.csv(file = "../output/ADFG_SE_AK_pot_surveys/cleaned_data/crab_data/BCS_examined_crab_with_temperature.csv")

# Examine things related to location (1 row per pot deployment)
pot_dat <- read.csv(file = "../output/ADFG_SE_AK_pot_surveys/cleaned_data/pot_data_with_temperature.csv")
```


# Exploring the Pot Data

```{r}
#### Temperature and Site --------------------------
exper <- pot_dat %>%
  group_by(Location) %>%
  summarize(temp_avg  = mean(temp),
            temp_sd = sd(temp),
            n_pots = n()) %>%
  ungroup()

ggplot(exper) +
  geom_bar(aes(x = Location, y = temp_avg),
           stat = "identity") +
  geom_errorbar(aes(x = Location, ymin = temp_avg - temp_sd, ymax = temp_avg + temp_sd)) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))

# Alright, neat little graph here! Looks like most sites have a broadly similar range of temperatures, with some exceptions. Port Camden in particular seems to be substantially higher than practically every other site!

# Let's look at how temperature relates to year!
exper <- pot_dat %>%
  group_by(Year) %>%
  summarize(temp_avg  = mean(temp),
            temp_sd = sd(temp),
            n_pots = n()) %>%
  ungroup()

ggplot(exper) +
  geom_bar(aes(x = Year, y = temp_avg),
           stat = "identity") +
  geom_errorbar(aes(x = Year, ymin = temp_avg - temp_sd, ymax = temp_avg + temp_sd)) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))

#### Temp vs. time of year ----------------------------
# We'll use Julian day for this
pot_dat$Jul.Day <- as.Date(pot_dat$Time.Hauled)
pot_dat$Jul.Day <- yday(pot_dat$Jul.Day)

# First, a scatterplot of all temps and Julian days
ggplot(pot_dat, aes(x = Jul.Day, y = temp)) +
  geom_point()

# Alright, we can see an overall gentle curve, along with the timing of each survey (Tanner vs. RKC)

# Now, let's make a line plot!
exper <- pot_dat %>%
  group_by(Jul.Day) %>%
  summarize(temp_avg = mean(temp))

ggplot(exper, aes( x= Jul.Day, y = temp_avg)) +
  geom_line() +
  geom_point()

#### Temp vs. depth ---------------------------
# We'll look at the average depth of each site vs. the average temp
exper <- pot_dat %>%
  group_by(Location) %>%
  summarize(depth_avg  = mean(Depth.Fathoms),
            temp_avg = mean(temp),
            temp_sd = sd(temp),
            depth_sd = sd(Depth.Fathoms),
            n_pots = n()) %>%
  ungroup()

# First, depth of each site
ggplot(exper) +
  geom_bar(aes(x = Location, y = depth_avg),
           stat = "identity") +
  geom_errorbar(aes(x = Location, ymin = depth_avg - depth_sd, ymax = depth_avg + depth_sd)) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))

# Now, depth vs. temperature
ggplot(exper, aes(x = depth_avg, y = temp_avg)) +
  geom_point()

# Now let's switch it up to maximum depth (more of an examination of the water bodies) and temperature
exper <- pot_dat %>%
  group_by(Location) %>%
  summarize(depth_max  = max(Depth.Fathoms),
            temp_avg = mean(temp),
            temp_sd = sd(temp),
            n_pots = n(),
            avg_day = mean(Jul.Day),
            day_sd = sd(Jul.Day))%>%
  ungroup()

# First, depth of each site
ggplot(exper) +
  geom_bar(aes(x = Location, y = depth_max),
           stat = "identity") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))

# Now, depth vs. temperature
ggplot(exper, aes(x = depth_max, y = temp_avg)) +
  geom_point()

# Hmm, doesn't seem to be a super strong relationship there. Neat! Note though, this is averaged by site

# What about average date each site was sampled?
ggplot(exper) +
  geom_bar(aes(x = Location, y = avg_day),
           stat = "identity") +
  geom_errorbar(aes(x = Location, ymin = avg_day - day_sd, ymax = avg_day + day_sd)) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))  

# Hmm, alright. Looks like a few of the sites are specific to the later survey, while most aren't. 
# Let's verify this with a table
table(pot_dat$Location, pot_dat$Project)

# Interesting! Alright, looks like surveys fall into the following buckets:
# Tanner crab only:
#       - Glacier Bay
#       - Icy Strait
#       - Port Camden
#       - Stephens Passage
#       - Thomas Bay
# Both Tanner and RKC survey:
#       - Hoklham Bay
# RKC only:
#       - All other surveys

# That might complicate things a bit - disentangling the effect of location and date - but hey, ah well.

# One thing I'm concerned about is a strong relationship between temperature and date. Let's figure that out
cor(pot_dat
    $temp, pot_dat$Jul.Day)
# Alright, that's acceptable! Let's just graph it too
ggplot(pot_dat, aes(x = Jul.Day, y = temp)) +
  geom_point()
# Clearly a warming trend over time, but it does flatten by October (realistically, peaks and goes back down)

# Let's also double check on latitude. Almost definitely fine, because our spatial scale is small, but hey
ggplot(pot_dat, aes(x = Latitude.Decimal.Degrees, y = temp)) +
  geom_point()

beep(8)

# Ooh, that's not good, we've got a few pots with crazy outlier values. Alright, let's switch over to cleaning up our crab data, since that's what we really care about!
```

#### Cleaning crab data

The plan is to go through each column one by one and clean up the dataframe in that order

```{r}
# Look at the columns to be examined
names(crab_dat)

# Check each column for NAs
colSums(is.na(crab_dat))

# Alright, some NAs in columns are understandable (male crab won't have egg condition values, for instance), others aren't. 

# Let's remove NA values in columns where we care about them

# Sex
crab_dat <- crab_dat[!is.na(crab_dat$Sex), ]

# Width
crab_dat <- crab_dat[!is.na(crab_dat$Width.Millimeters), ]

# Shell condition
crab_dat <- crab_dat[!is.na(crab_dat$Shell.Condition), ]

# Leg condition
crab_dat <- crab_dat[!is.na(crab_dat$Leg.Condition), ]

# Depth
crab_dat <- crab_dat[!is.na(crab_dat$Depth.Fathoms), ]

# Check again
colSums(is.na(crab_dat))

# Alright, only remaining NAs are in male-specific columns (chela height) or female-specific (egg-related)

# Proceed with examining each column specifically in detail

#### Year ---------------------------------------
table(crab_dat$Year)
any(is.na(crab_dat$Year))


#Let's eliminate everything with a latitude < 50 or > 60 in our crab_dat, which is what we really care about!
crab_dat <- crab_dat %>%
  filter(Latitude.Decimal.Degrees > 50) %>%
  filter(Latitude.Decimal.Degrees < 60)

# Let's do the same with longitude, eliminating all with a latitude < -140 or > -130
crab_dat <- crab_dat %>%
  filter(Longitude.Decimal.Degrees > -140) %>%
  filter(Longitude.Decimal.Degrees < -130)






```